{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Linear Regression and KNN - Train/Test Split\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We've discussed overfitting in the context of bias and variance, and we've touched on some techniques, such as regularization, that are used to avoid overfitting (but haven't practiced them yet). In this lesson we'll discuss a fundamental method for avoiding overfitting that is commonly referred to as _train/test split_ validation. \n",
    "\n",
    "The idea is similar to something called \"cross-validation\" — in fact, it is a type of cross-validation — in that we split the data set into two subsets:\n",
    "* A subset on which to train our model.\n",
    "* A subset on which to test our model's predictions.\n",
    "\n",
    "This serves two useful purposes:\n",
    "* We prevent overfitting by not using all of the data.\n",
    "* We have some remaining data we can use to evaluate our model.\n",
    "\n",
    "While this may seem like a relatively simple idea, **there are some caveats** to putting it into practice. For example, if you are not careful, it is easy to take a non-random split. Suppose we have salary data on technical professionals that is composed of 80 percent data from California and 20 percent data from elsewhere and is sorted by state. If we split our data into 80 percent training data and 20 percent testing data, we might inadvertantly select all the California data to train and all the non-California data to test. In this case we've still overfit on our data set because we did not sufficiently randomize the data.\n",
    "\n",
    "In a situation like this we can use _k-fold cross-validation_, which is the same idea applied to more than two subsets. In particular, we partition our data into $k$ subsets and train on $k-1$ one of them, holding the last slice for testing. We can do this for each of the possible $k-1$ subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Practice\n",
    "\n",
    "Ultimately we use a test-training split to compare multiple models on the same data set. This could be comparisons of two linear models or of completely different models on the same data.\n",
    "\n",
    "For your independent practice, fit three different models on the Boston housing data. For example, you could pick three different subsets of variables, one or more polynomial models, or any other model you'd like. We have done this already with one variable, so could chose another.\n",
    "\n",
    "### Here's What We Will Be Doing:\n",
    "\n",
    "* Working with Boston housing data to predict the value of a home\n",
    "* Create a test-train split of the data.\n",
    "* Train each of your models on the training data.\n",
    "* Evaluate each of the models on the test data.\n",
    "* Rank the models by how well they score on the testing data set.\n",
    "\n",
    "**Then, try k-folds.**\n",
    "\n",
    "* Try a few different splits of data for the same models.\n",
    "* Perform a k-fold cross-validation and use the cross-validation scores to compare your models. Did this change your rankings?\n",
    "\n",
    "**Be sure to provide interpretation for your results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that k-fold cross-validation creates a hold portion of your data set for each iteration of training and validating:\n",
    "\n",
    "![](http://i.imgur.com/0PFrPXJ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Use Case\n",
    "\n",
    "In this given task, you will be asked to model the median home price of various houses across U.S. Census tracts in the city of Boston. This is a probable use case: We are predicting a continuous, numeric output (price) based on a combination of discrete features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "X = pd.DataFrame(boston.data,\n",
    "                 columns=boston.feature_names)\n",
    "y = pd.DataFrame(boston.target,\n",
    "                 columns=['MEDV']) #choose another one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(boston['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clean Up Data and Perform Exporatory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Boston data is from scikit-learn, so it ought to be pretty clean, but we should always perform exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory data analysis.\n",
    "\n",
    "# Include: total nulls, index, data types, shape, summary statistics, and the number of unique values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRIM       0\n",
       "ZN         0\n",
       "INDUS      0\n",
       "CHAS       0\n",
       "NOX        0\n",
       "RM         0\n",
       "AGE        0\n",
       "DIS        0\n",
       "RAD        0\n",
       "TAX        0\n",
       "PTRATIO    0\n",
       "B          0\n",
       "LSTAT      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 13 columns):\n",
      "CRIM       506 non-null float64\n",
      "ZN         506 non-null float64\n",
      "INDUS      506 non-null float64\n",
      "CHAS       506 non-null float64\n",
      "NOX        506 non-null float64\n",
      "RM         506 non-null float64\n",
      "AGE        506 non-null float64\n",
      "DIS        506 non-null float64\n",
      "RAD        506 non-null float64\n",
      "TAX        506 non-null float64\n",
      "PTRATIO    506 non-null float64\n",
      "B          506 non-null float64\n",
      "LSTAT      506 non-null float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 51.5 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of          CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS   RAD    TAX  \\\n",
       "0     0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900   1.0  296.0   \n",
       "1     0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671   2.0  242.0   \n",
       "2     0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671   2.0  242.0   \n",
       "3     0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622   3.0  222.0   \n",
       "4     0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622   3.0  222.0   \n",
       "5     0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622   3.0  222.0   \n",
       "6     0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605   5.0  311.0   \n",
       "7     0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505   5.0  311.0   \n",
       "8     0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821   5.0  311.0   \n",
       "9     0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921   5.0  311.0   \n",
       "10    0.22489  12.5   7.87   0.0  0.524  6.377   94.3  6.3467   5.0  311.0   \n",
       "11    0.11747  12.5   7.87   0.0  0.524  6.009   82.9  6.2267   5.0  311.0   \n",
       "12    0.09378  12.5   7.87   0.0  0.524  5.889   39.0  5.4509   5.0  311.0   \n",
       "13    0.62976   0.0   8.14   0.0  0.538  5.949   61.8  4.7075   4.0  307.0   \n",
       "14    0.63796   0.0   8.14   0.0  0.538  6.096   84.5  4.4619   4.0  307.0   \n",
       "15    0.62739   0.0   8.14   0.0  0.538  5.834   56.5  4.4986   4.0  307.0   \n",
       "16    1.05393   0.0   8.14   0.0  0.538  5.935   29.3  4.4986   4.0  307.0   \n",
       "17    0.78420   0.0   8.14   0.0  0.538  5.990   81.7  4.2579   4.0  307.0   \n",
       "18    0.80271   0.0   8.14   0.0  0.538  5.456   36.6  3.7965   4.0  307.0   \n",
       "19    0.72580   0.0   8.14   0.0  0.538  5.727   69.5  3.7965   4.0  307.0   \n",
       "20    1.25179   0.0   8.14   0.0  0.538  5.570   98.1  3.7979   4.0  307.0   \n",
       "21    0.85204   0.0   8.14   0.0  0.538  5.965   89.2  4.0123   4.0  307.0   \n",
       "22    1.23247   0.0   8.14   0.0  0.538  6.142   91.7  3.9769   4.0  307.0   \n",
       "23    0.98843   0.0   8.14   0.0  0.538  5.813  100.0  4.0952   4.0  307.0   \n",
       "24    0.75026   0.0   8.14   0.0  0.538  5.924   94.1  4.3996   4.0  307.0   \n",
       "25    0.84054   0.0   8.14   0.0  0.538  5.599   85.7  4.4546   4.0  307.0   \n",
       "26    0.67191   0.0   8.14   0.0  0.538  5.813   90.3  4.6820   4.0  307.0   \n",
       "27    0.95577   0.0   8.14   0.0  0.538  6.047   88.8  4.4534   4.0  307.0   \n",
       "28    0.77299   0.0   8.14   0.0  0.538  6.495   94.4  4.4547   4.0  307.0   \n",
       "29    1.00245   0.0   8.14   0.0  0.538  6.674   87.3  4.2390   4.0  307.0   \n",
       "..        ...   ...    ...   ...    ...    ...    ...     ...   ...    ...   \n",
       "476   4.87141   0.0  18.10   0.0  0.614  6.484   93.6  2.3053  24.0  666.0   \n",
       "477  15.02340   0.0  18.10   0.0  0.614  5.304   97.3  2.1007  24.0  666.0   \n",
       "478  10.23300   0.0  18.10   0.0  0.614  6.185   96.7  2.1705  24.0  666.0   \n",
       "479  14.33370   0.0  18.10   0.0  0.614  6.229   88.0  1.9512  24.0  666.0   \n",
       "480   5.82401   0.0  18.10   0.0  0.532  6.242   64.7  3.4242  24.0  666.0   \n",
       "481   5.70818   0.0  18.10   0.0  0.532  6.750   74.9  3.3317  24.0  666.0   \n",
       "482   5.73116   0.0  18.10   0.0  0.532  7.061   77.0  3.4106  24.0  666.0   \n",
       "483   2.81838   0.0  18.10   0.0  0.532  5.762   40.3  4.0983  24.0  666.0   \n",
       "484   2.37857   0.0  18.10   0.0  0.583  5.871   41.9  3.7240  24.0  666.0   \n",
       "485   3.67367   0.0  18.10   0.0  0.583  6.312   51.9  3.9917  24.0  666.0   \n",
       "486   5.69175   0.0  18.10   0.0  0.583  6.114   79.8  3.5459  24.0  666.0   \n",
       "487   4.83567   0.0  18.10   0.0  0.583  5.905   53.2  3.1523  24.0  666.0   \n",
       "488   0.15086   0.0  27.74   0.0  0.609  5.454   92.7  1.8209   4.0  711.0   \n",
       "489   0.18337   0.0  27.74   0.0  0.609  5.414   98.3  1.7554   4.0  711.0   \n",
       "490   0.20746   0.0  27.74   0.0  0.609  5.093   98.0  1.8226   4.0  711.0   \n",
       "491   0.10574   0.0  27.74   0.0  0.609  5.983   98.8  1.8681   4.0  711.0   \n",
       "492   0.11132   0.0  27.74   0.0  0.609  5.983   83.5  2.1099   4.0  711.0   \n",
       "493   0.17331   0.0   9.69   0.0  0.585  5.707   54.0  2.3817   6.0  391.0   \n",
       "494   0.27957   0.0   9.69   0.0  0.585  5.926   42.6  2.3817   6.0  391.0   \n",
       "495   0.17899   0.0   9.69   0.0  0.585  5.670   28.8  2.7986   6.0  391.0   \n",
       "496   0.28960   0.0   9.69   0.0  0.585  5.390   72.9  2.7986   6.0  391.0   \n",
       "497   0.26838   0.0   9.69   0.0  0.585  5.794   70.6  2.8927   6.0  391.0   \n",
       "498   0.23912   0.0   9.69   0.0  0.585  6.019   65.3  2.4091   6.0  391.0   \n",
       "499   0.17783   0.0   9.69   0.0  0.585  5.569   73.5  2.3999   6.0  391.0   \n",
       "500   0.22438   0.0   9.69   0.0  0.585  6.027   79.7  2.4982   6.0  391.0   \n",
       "501   0.06263   0.0  11.93   0.0  0.573  6.593   69.1  2.4786   1.0  273.0   \n",
       "502   0.04527   0.0  11.93   0.0  0.573  6.120   76.7  2.2875   1.0  273.0   \n",
       "503   0.06076   0.0  11.93   0.0  0.573  6.976   91.0  2.1675   1.0  273.0   \n",
       "504   0.10959   0.0  11.93   0.0  0.573  6.794   89.3  2.3889   1.0  273.0   \n",
       "505   0.04741   0.0  11.93   0.0  0.573  6.030   80.8  2.5050   1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "0       15.3  396.90   4.98  \n",
       "1       17.8  396.90   9.14  \n",
       "2       17.8  392.83   4.03  \n",
       "3       18.7  394.63   2.94  \n",
       "4       18.7  396.90   5.33  \n",
       "5       18.7  394.12   5.21  \n",
       "6       15.2  395.60  12.43  \n",
       "7       15.2  396.90  19.15  \n",
       "8       15.2  386.63  29.93  \n",
       "9       15.2  386.71  17.10  \n",
       "10      15.2  392.52  20.45  \n",
       "11      15.2  396.90  13.27  \n",
       "12      15.2  390.50  15.71  \n",
       "13      21.0  396.90   8.26  \n",
       "14      21.0  380.02  10.26  \n",
       "15      21.0  395.62   8.47  \n",
       "16      21.0  386.85   6.58  \n",
       "17      21.0  386.75  14.67  \n",
       "18      21.0  288.99  11.69  \n",
       "19      21.0  390.95  11.28  \n",
       "20      21.0  376.57  21.02  \n",
       "21      21.0  392.53  13.83  \n",
       "22      21.0  396.90  18.72  \n",
       "23      21.0  394.54  19.88  \n",
       "24      21.0  394.33  16.30  \n",
       "25      21.0  303.42  16.51  \n",
       "26      21.0  376.88  14.81  \n",
       "27      21.0  306.38  17.28  \n",
       "28      21.0  387.94  12.80  \n",
       "29      21.0  380.23  11.98  \n",
       "..       ...     ...    ...  \n",
       "476     20.2  396.21  18.68  \n",
       "477     20.2  349.48  24.91  \n",
       "478     20.2  379.70  18.03  \n",
       "479     20.2  383.32  13.11  \n",
       "480     20.2  396.90  10.74  \n",
       "481     20.2  393.07   7.74  \n",
       "482     20.2  395.28   7.01  \n",
       "483     20.2  392.92  10.42  \n",
       "484     20.2  370.73  13.34  \n",
       "485     20.2  388.62  10.58  \n",
       "486     20.2  392.68  14.98  \n",
       "487     20.2  388.22  11.45  \n",
       "488     20.1  395.09  18.06  \n",
       "489     20.1  344.05  23.97  \n",
       "490     20.1  318.43  29.68  \n",
       "491     20.1  390.11  18.07  \n",
       "492     20.1  396.90  13.35  \n",
       "493     19.2  396.90  12.01  \n",
       "494     19.2  396.90  13.59  \n",
       "495     19.2  393.29  17.60  \n",
       "496     19.2  396.90  21.14  \n",
       "497     19.2  396.90  14.10  \n",
       "498     19.2  396.90  12.92  \n",
       "499     19.2  395.77  15.10  \n",
       "500     19.2  396.90  14.33  \n",
       "501     21.0  391.99   9.67  \n",
       "502     21.0  396.90   9.08  \n",
       "503     21.0  396.90   5.64  \n",
       "504     21.0  393.45   6.48  \n",
       "505     21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CRIM ---\n",
      "(-0.08360000000000001, 29.663]    498\n",
      "(29.663, 59.32]                     5\n",
      "(59.32, 88.976]                     3\n",
      "Name: CRIM, dtype: int64\n",
      "--- ZN ---\n",
      "(-0.101, 33.333]    442\n",
      "(66.667, 100.0]      35\n",
      "(33.333, 66.667]     29\n",
      "Name: ZN, dtype: int64\n",
      "--- INDUS ---\n",
      "(0.432, 9.553]     250\n",
      "(9.553, 18.647]    199\n",
      "(18.647, 27.74]     57\n",
      "Name: INDUS, dtype: int64\n",
      "--- CHAS ---\n",
      "(-0.002, 0.333]    471\n",
      "(0.667, 1.0]        35\n",
      "(0.333, 0.667]       0\n",
      "Name: CHAS, dtype: int64\n",
      "--- NOX ---\n",
      "(0.384, 0.547]    293\n",
      "(0.547, 0.709]    152\n",
      "(0.709, 0.871]     61\n",
      "Name: NOX, dtype: int64\n",
      "--- RM ---\n",
      "(5.301, 7.04]     422\n",
      "(7.04, 8.78]       60\n",
      "(3.555, 5.301]     24\n",
      "Name: RM, dtype: int64\n",
      "--- AGE ---\n",
      "(67.633, 100.0]     296\n",
      "(35.267, 67.633]    119\n",
      "(2.802, 35.267]      91\n",
      "Name: AGE, dtype: int64\n",
      "--- DIS ---\n",
      "(1.118, 4.795]     365\n",
      "(4.795, 8.461]     125\n",
      "(8.461, 12.126]     16\n",
      "Name: DIS, dtype: int64\n",
      "--- RAD ---\n",
      "(0.976, 8.667]     374\n",
      "(16.333, 24.0]     132\n",
      "(8.667, 16.333]      0\n",
      "Name: RAD, dtype: int64\n",
      "--- TAX ---\n",
      "(186.475, 361.667]    273\n",
      "(536.333, 711.0]      137\n",
      "(361.667, 536.333]     96\n",
      "Name: TAX, dtype: int64\n",
      "--- PTRATIO ---\n",
      "(18.867, 22.0]      260\n",
      "(15.733, 18.867]    168\n",
      "(12.59, 15.733]      78\n",
      "Name: PTRATIO, dtype: int64\n",
      "--- B ---\n",
      "(264.707, 396.9]      458\n",
      "(-0.0776, 132.513]     36\n",
      "(132.513, 264.707]     12\n",
      "Name: B, dtype: int64\n",
      "--- LSTAT ---\n",
      "(1.693, 13.81]    313\n",
      "(13.81, 25.89]    162\n",
      "(25.89, 37.97]     31\n",
      "Name: LSTAT, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for i in X.columns:\n",
    "    print(\"--- %s ---\" % i)\n",
    "    print(X[i].value_counts(bins = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using scikit-learn Linear Regression¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Pick 3-4 predictors (i.e. CRIM, ZN, etc...) that you will use to predict our target variable, MEDV.\n",
    "Score and plot your predictions. What do these results tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. INITIALIZING THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. FITTING THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of rooms, accessibility to highways, crime and distance to employment\n",
    "X1 = X[[\"RM\",\"RAD\",\"CRIM\",\"DIS\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = linreg.fit(X1,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. INSPECT TRAINING MODEL**\n",
    "\n",
    "Storing coefficients and Intercept. (And plot if possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression COEFFICIENTS\n",
      "[[ 8.26162004 -0.16929889 -0.16748505 -0.08017929]]\n"
     ]
    }
   ],
   "source": [
    "coefficients = linreg.coef_\n",
    "print(\"Linear Regression COEFFICIENTS\")\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that :\n",
    "- Accessibility to Highways, crime and distance to employment have a minor inverse relation to the price.\n",
    "- The umber of rooms seems to have a greater positive impact on the price of the house. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression INTERCEPT\n",
      "[-26.86225613]\n"
     ]
    }
   ],
   "source": [
    "intercept = linreg.intercept_\n",
    "print(\"Linear Regression INTERCEPT\")\n",
    "print(intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linreg.predict(X1) #essentially this is the model ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. EVALUATE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-06f899884e48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msum_sq_residuals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_sq_residuals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "## I couldn't get this to work . YES, it's the same as calculating the below but on a \"manual way\"\n",
    "sum_sq_residuals= 0\n",
    "for i in range(0,len(y)):\n",
    "    print(i)\n",
    "    sum_sq_residuals += (y[i] - y_pred[i])**2\n",
    "    \n",
    "print(sum_sq_residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating **MEAN SQUARED ERROR (MSE)** and **ROOT MEAN QUARED ERROR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 37.458619\n",
      "Root mean squared error: 6.120345\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_pred, y)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print('Mean squared error: %f'% mse)\n",
    "print('Root mean squared error: %f'% rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty bad numbers overall --> possible causes:\n",
    "    - factors are not relevant\n",
    "    - Linear regression model might not be the best for this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AFTER THOUGHT !!** \n",
    "The below should probably be done earlier to choose the factors to incorporate into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sns.pairplot(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = X.merge(y, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CRIM</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.200469</td>\n",
       "      <td>0.406583</td>\n",
       "      <td>-0.055892</td>\n",
       "      <td>0.420972</td>\n",
       "      <td>-0.219247</td>\n",
       "      <td>0.352734</td>\n",
       "      <td>-0.379670</td>\n",
       "      <td>0.625505</td>\n",
       "      <td>0.582764</td>\n",
       "      <td>0.289946</td>\n",
       "      <td>-0.385064</td>\n",
       "      <td>0.455621</td>\n",
       "      <td>-0.388305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN</th>\n",
       "      <td>-0.200469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.533828</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>-0.412995</td>\n",
       "      <td>0.360445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS</th>\n",
       "      <td>0.406583</td>\n",
       "      <td>-0.533828</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.644779</td>\n",
       "      <td>-0.708027</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>0.720760</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>-0.356977</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>-0.483725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS</th>\n",
       "      <td>-0.055892</td>\n",
       "      <td>-0.042697</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>0.175260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>0.420972</td>\n",
       "      <td>-0.516604</td>\n",
       "      <td>0.763651</td>\n",
       "      <td>0.091203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.380051</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>-0.427321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM</th>\n",
       "      <td>-0.219247</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>-0.391676</td>\n",
       "      <td>0.091251</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>0.695360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>0.352734</td>\n",
       "      <td>-0.569537</td>\n",
       "      <td>0.644779</td>\n",
       "      <td>0.086518</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>-0.240265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.273534</td>\n",
       "      <td>0.602339</td>\n",
       "      <td>-0.376955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS</th>\n",
       "      <td>-0.379670</td>\n",
       "      <td>0.664408</td>\n",
       "      <td>-0.708027</td>\n",
       "      <td>-0.099176</td>\n",
       "      <td>-0.769230</td>\n",
       "      <td>0.205246</td>\n",
       "      <td>-0.747881</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>-0.496996</td>\n",
       "      <td>0.249929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD</th>\n",
       "      <td>0.625505</td>\n",
       "      <td>-0.311948</td>\n",
       "      <td>0.595129</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>-0.209847</td>\n",
       "      <td>0.456022</td>\n",
       "      <td>-0.494588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>-0.444413</td>\n",
       "      <td>0.488676</td>\n",
       "      <td>-0.381626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX</th>\n",
       "      <td>0.582764</td>\n",
       "      <td>-0.314563</td>\n",
       "      <td>0.720760</td>\n",
       "      <td>-0.035587</td>\n",
       "      <td>0.668023</td>\n",
       "      <td>-0.292048</td>\n",
       "      <td>0.506456</td>\n",
       "      <td>-0.534432</td>\n",
       "      <td>0.910228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>-0.441808</td>\n",
       "      <td>0.543993</td>\n",
       "      <td>-0.468536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO</th>\n",
       "      <td>0.289946</td>\n",
       "      <td>-0.391679</td>\n",
       "      <td>0.383248</td>\n",
       "      <td>-0.121515</td>\n",
       "      <td>0.188933</td>\n",
       "      <td>-0.355501</td>\n",
       "      <td>0.261515</td>\n",
       "      <td>-0.232471</td>\n",
       "      <td>0.464741</td>\n",
       "      <td>0.460853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.177383</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>-0.507787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>-0.385064</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>-0.356977</td>\n",
       "      <td>0.048788</td>\n",
       "      <td>-0.380051</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>-0.273534</td>\n",
       "      <td>0.291512</td>\n",
       "      <td>-0.444413</td>\n",
       "      <td>-0.441808</td>\n",
       "      <td>-0.177383</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.366087</td>\n",
       "      <td>0.333461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT</th>\n",
       "      <td>0.455621</td>\n",
       "      <td>-0.412995</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>0.602339</td>\n",
       "      <td>-0.496996</td>\n",
       "      <td>0.488676</td>\n",
       "      <td>0.543993</td>\n",
       "      <td>0.374044</td>\n",
       "      <td>-0.366087</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.737663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDV</th>\n",
       "      <td>-0.388305</td>\n",
       "      <td>0.360445</td>\n",
       "      <td>-0.483725</td>\n",
       "      <td>0.175260</td>\n",
       "      <td>-0.427321</td>\n",
       "      <td>0.695360</td>\n",
       "      <td>-0.376955</td>\n",
       "      <td>0.249929</td>\n",
       "      <td>-0.381626</td>\n",
       "      <td>-0.468536</td>\n",
       "      <td>-0.507787</td>\n",
       "      <td>0.333461</td>\n",
       "      <td>-0.737663</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "CRIM     1.000000 -0.200469  0.406583 -0.055892  0.420972 -0.219247  0.352734   \n",
       "ZN      -0.200469  1.000000 -0.533828 -0.042697 -0.516604  0.311991 -0.569537   \n",
       "INDUS    0.406583 -0.533828  1.000000  0.062938  0.763651 -0.391676  0.644779   \n",
       "CHAS    -0.055892 -0.042697  0.062938  1.000000  0.091203  0.091251  0.086518   \n",
       "NOX      0.420972 -0.516604  0.763651  0.091203  1.000000 -0.302188  0.731470   \n",
       "RM      -0.219247  0.311991 -0.391676  0.091251 -0.302188  1.000000 -0.240265   \n",
       "AGE      0.352734 -0.569537  0.644779  0.086518  0.731470 -0.240265  1.000000   \n",
       "DIS     -0.379670  0.664408 -0.708027 -0.099176 -0.769230  0.205246 -0.747881   \n",
       "RAD      0.625505 -0.311948  0.595129 -0.007368  0.611441 -0.209847  0.456022   \n",
       "TAX      0.582764 -0.314563  0.720760 -0.035587  0.668023 -0.292048  0.506456   \n",
       "PTRATIO  0.289946 -0.391679  0.383248 -0.121515  0.188933 -0.355501  0.261515   \n",
       "B       -0.385064  0.175520 -0.356977  0.048788 -0.380051  0.128069 -0.273534   \n",
       "LSTAT    0.455621 -0.412995  0.603800 -0.053929  0.590879 -0.613808  0.602339   \n",
       "MEDV    -0.388305  0.360445 -0.483725  0.175260 -0.427321  0.695360 -0.376955   \n",
       "\n",
       "              DIS       RAD       TAX   PTRATIO         B     LSTAT      MEDV  \n",
       "CRIM    -0.379670  0.625505  0.582764  0.289946 -0.385064  0.455621 -0.388305  \n",
       "ZN       0.664408 -0.311948 -0.314563 -0.391679  0.175520 -0.412995  0.360445  \n",
       "INDUS   -0.708027  0.595129  0.720760  0.383248 -0.356977  0.603800 -0.483725  \n",
       "CHAS    -0.099176 -0.007368 -0.035587 -0.121515  0.048788 -0.053929  0.175260  \n",
       "NOX     -0.769230  0.611441  0.668023  0.188933 -0.380051  0.590879 -0.427321  \n",
       "RM       0.205246 -0.209847 -0.292048 -0.355501  0.128069 -0.613808  0.695360  \n",
       "AGE     -0.747881  0.456022  0.506456  0.261515 -0.273534  0.602339 -0.376955  \n",
       "DIS      1.000000 -0.494588 -0.534432 -0.232471  0.291512 -0.496996  0.249929  \n",
       "RAD     -0.494588  1.000000  0.910228  0.464741 -0.444413  0.488676 -0.381626  \n",
       "TAX     -0.534432  0.910228  1.000000  0.460853 -0.441808  0.543993 -0.468536  \n",
       "PTRATIO -0.232471  0.464741  0.460853  1.000000 -0.177383  0.374044 -0.507787  \n",
       "B        0.291512 -0.444413 -0.441808 -0.177383  1.000000 -0.366087  0.333461  \n",
       "LSTAT   -0.496996  0.488676  0.543993  0.374044 -0.366087  1.000000 -0.737663  \n",
       "MEDV     0.249929 -0.381626 -0.468536 -0.507787  0.333461 -0.737663  1.000000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(temp_df.corr());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should have pobably chosen a different set of factors that berret correlate to my target variable such as:\n",
    "    - \"LSTAT\" (-0.737663)\n",
    "    - \"RM\" (0.695360)\n",
    "    - \"PTRATIO\" (-0.507787)\n",
    "    - \"INDUS\" (-0.483725) \n",
    "\n",
    "\n",
    "+ LSTAT:    % lower status of the population\n",
    "+ RM:       average number of rooms per dwelling\n",
    "+ PTRATIO:  pupil-teacher ratio by town\n",
    "+ INDUS:    proportion of non-retail business acres per town"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Try 70/30 and 90/10 train/test splits (70% of the data for training - 30% for testing, then 90% for training - 10% for testing)\n",
    "Score and plot. How do your metrics change? What does this tell us about the size of training/testing splits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the 4 measures that most correlate. (ABOVE)\n",
    "X2 = X[[\"LSTAT\",\"RM\",\"PTRATIO\",\"INDUS\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### **Model with 70-30**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X70_train, X30_test, y70_train, y30_test = train_test_split(X2,y,test_size=30, train_size= 70 ,random_state=24) \n",
    "#NOT 42, just to annoy you guys :P "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Initialize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "linreg2 = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Fitting the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model70_30 = linreg2.fit(X70_train,y70_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Inspecting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression COEFFICIENTS\n",
      "[[-0.24826193  5.34687538 -1.34728542 -0.09954324]]\n"
     ]
    }
   ],
   "source": [
    "coefficients2 = linreg2.coef_\n",
    "print(\"Linear Regression COEFFICIENTS\")\n",
    "print(coefficients2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression INTERCEPT\n",
      "[17.45053032]\n"
     ]
    }
   ],
   "source": [
    "intercept2 = linreg2.intercept_\n",
    "print(\"Linear Regression INTERCEPT\")\n",
    "print(intercept2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred2 = linreg2.predict(X30_test) #predicting on the TEST data ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 56.593029\n",
      "Root mean squared error: 7.522834\n"
     ]
    }
   ],
   "source": [
    "mse2 = mean_squared_error(y_pred2, y30_test)\n",
    "rmse2 = np.sqrt(mse2)\n",
    "\n",
    "print('Mean squared error: %f'% mse2)\n",
    "print('Root mean squared error: %f'% rmse2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09704158128462104"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model70_30.score(X30_test, y30_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model with 90-10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X90_train, X10_test, y90_train, y10_test = train_test_split(X2,y,test_size=10, train_size= 90 ,random_state=24) #NOT 42, just to annoy you guys :P "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg3 = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model90_10 = linreg3.fit(X90_train,y90_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspecting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression COEFFICIENTS\n",
      "[[-0.35496189  4.66524922 -1.35508434 -0.01133824]]\n"
     ]
    }
   ],
   "source": [
    "coefficients3 = linreg3.coef_\n",
    "print(\"Linear Regression COEFFICIENTS\")\n",
    "print(coefficients3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression INTERCEPT\n",
      "[22.27691774]\n"
     ]
    }
   ],
   "source": [
    "intercept3 = linreg3.intercept_\n",
    "print(\"Linear Regression INTERCEPT\")\n",
    "print(intercept3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3 = linreg3.predict(X10_test) #predicting on the TEST data ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 117.161292\n",
      "Root mean squared error: 10.824107\n"
     ]
    }
   ],
   "source": [
    "mse3 = mean_squared_error(y_pred3, y10_test)\n",
    "rmse3 = np.sqrt(mse3)\n",
    "\n",
    "print('Mean squared error: %f'% mse3)\n",
    "print('Root mean squared error: %f'% rmse3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0918184568537792"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model90_10.score(X10_test, y10_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing both models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use k-fold cross validation varying the number of folds from 5 to 10\n",
    "What seems optimal? How do your scores change? What is the variance like? Try different folds to get a sense of how this impacts your score. What are the tradeoffs associated with choosing the number of folds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(X,y):\n",
    "    X_train, X_test = X2.iloc[train_index], X2.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>RM</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>INDUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.98</td>\n",
       "      <td>6.575</td>\n",
       "      <td>15.3</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.14</td>\n",
       "      <td>6.421</td>\n",
       "      <td>17.8</td>\n",
       "      <td>7.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.03</td>\n",
       "      <td>7.185</td>\n",
       "      <td>17.8</td>\n",
       "      <td>7.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.33</td>\n",
       "      <td>7.147</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.21</td>\n",
       "      <td>6.430</td>\n",
       "      <td>18.7</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12.43</td>\n",
       "      <td>6.012</td>\n",
       "      <td>15.2</td>\n",
       "      <td>7.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>29.93</td>\n",
       "      <td>5.631</td>\n",
       "      <td>15.2</td>\n",
       "      <td>7.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17.10</td>\n",
       "      <td>6.004</td>\n",
       "      <td>15.2</td>\n",
       "      <td>7.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13.27</td>\n",
       "      <td>6.009</td>\n",
       "      <td>15.2</td>\n",
       "      <td>7.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15.71</td>\n",
       "      <td>5.889</td>\n",
       "      <td>15.2</td>\n",
       "      <td>7.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.26</td>\n",
       "      <td>6.096</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8.47</td>\n",
       "      <td>5.834</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14.67</td>\n",
       "      <td>5.990</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11.69</td>\n",
       "      <td>5.456</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.02</td>\n",
       "      <td>5.570</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>13.83</td>\n",
       "      <td>5.965</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>18.72</td>\n",
       "      <td>6.142</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>19.88</td>\n",
       "      <td>5.813</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>16.30</td>\n",
       "      <td>5.924</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14.81</td>\n",
       "      <td>5.813</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>17.28</td>\n",
       "      <td>6.047</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>12.80</td>\n",
       "      <td>6.495</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>11.98</td>\n",
       "      <td>6.674</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>22.60</td>\n",
       "      <td>5.713</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>13.04</td>\n",
       "      <td>6.072</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>18.35</td>\n",
       "      <td>5.701</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>20.34</td>\n",
       "      <td>6.096</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>9.68</td>\n",
       "      <td>5.933</td>\n",
       "      <td>19.2</td>\n",
       "      <td>5.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>11.41</td>\n",
       "      <td>5.841</td>\n",
       "      <td>19.2</td>\n",
       "      <td>5.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>8.77</td>\n",
       "      <td>5.850</td>\n",
       "      <td>19.2</td>\n",
       "      <td>5.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>14.13</td>\n",
       "      <td>5.759</td>\n",
       "      <td>20.2</td>\n",
       "      <td>18.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>17.15</td>\n",
       "      <td>5.952</td>\n",
       "      <td>20.2</td>\n",
       "      <td>18.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>18.13</td>\n",
       "      <td>5.926</td>\n",
       "      <td>20.2</td>\n",
       "      <td>18.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>14.76</td>\n",
       "      <td>5.713</td>\n",
       "      <td>20.2</td>\n",
       "      <td>18.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>16.29</td>\n",
       "      <td>6.167</td>\n",
       "      <td>20.2</td>\n",
       "      <td>18.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>12.87</td>\n",
       "      <td>6.229</td>\n",
       "      <td>20.2</td>\n",
       "      <td>18.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>14.36</td>\n",
       "      <td>6.437</td>\n",
       "      <td>20.2</td>\n",
       "      <td>18.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>11.66</td>\n",
       "      <td>6.980</td>\n",
       "      <td>20.2</td>\n",
       "      <td>18.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>18.68</td>\n",
       "      <td>6.484</td>\n",
       "      <td>20.2</td>\n",
       "      <td>18.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>24.91</td>\n",
       "      <td>5.304</td>\n",
       "      <td>20.2</td>\n",
       "      <td>18.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>13.11</td>\n",
       "      <td>6.229</td>\n",
       "      <td>20.2</td>\n",
       "      <td>18.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>10.74</td>\n",
       "      <td>6.242</td>\n",
       "      <td>20.2</td>\n",
       "      <td>18.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>7.74</td>\n",
       "      <td>6.750</td>\n",
       "      <td>20.2</td>\n",
       "      <td>18.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>7.01</td>\n",
       "      <td>7.061</td>\n",
       "      <td>20.2</td>\n",
       "      <td>18.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>13.34</td>\n",
       "      <td>5.871</td>\n",
       "      <td>20.2</td>\n",
       "      <td>18.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>11.45</td>\n",
       "      <td>5.905</td>\n",
       "      <td>20.2</td>\n",
       "      <td>18.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>18.06</td>\n",
       "      <td>5.454</td>\n",
       "      <td>20.1</td>\n",
       "      <td>27.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>29.68</td>\n",
       "      <td>5.093</td>\n",
       "      <td>20.1</td>\n",
       "      <td>27.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>13.35</td>\n",
       "      <td>5.983</td>\n",
       "      <td>20.1</td>\n",
       "      <td>27.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>12.01</td>\n",
       "      <td>5.707</td>\n",
       "      <td>19.2</td>\n",
       "      <td>9.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>13.59</td>\n",
       "      <td>5.926</td>\n",
       "      <td>19.2</td>\n",
       "      <td>9.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>17.60</td>\n",
       "      <td>5.670</td>\n",
       "      <td>19.2</td>\n",
       "      <td>9.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>21.14</td>\n",
       "      <td>5.390</td>\n",
       "      <td>19.2</td>\n",
       "      <td>9.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>14.10</td>\n",
       "      <td>5.794</td>\n",
       "      <td>19.2</td>\n",
       "      <td>9.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>12.92</td>\n",
       "      <td>6.019</td>\n",
       "      <td>19.2</td>\n",
       "      <td>9.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>15.10</td>\n",
       "      <td>5.569</td>\n",
       "      <td>19.2</td>\n",
       "      <td>9.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>14.33</td>\n",
       "      <td>6.027</td>\n",
       "      <td>19.2</td>\n",
       "      <td>9.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>9.67</td>\n",
       "      <td>6.593</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>6.48</td>\n",
       "      <td>6.794</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>7.88</td>\n",
       "      <td>6.030</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>405 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     LSTAT     RM  PTRATIO  INDUS\n",
       "0     4.98  6.575     15.3   2.31\n",
       "1     9.14  6.421     17.8   7.07\n",
       "2     4.03  7.185     17.8   7.07\n",
       "4     5.33  7.147     18.7   2.18\n",
       "5     5.21  6.430     18.7   2.18\n",
       "6    12.43  6.012     15.2   7.87\n",
       "8    29.93  5.631     15.2   7.87\n",
       "9    17.10  6.004     15.2   7.87\n",
       "11   13.27  6.009     15.2   7.87\n",
       "12   15.71  5.889     15.2   7.87\n",
       "14   10.26  6.096     21.0   8.14\n",
       "15    8.47  5.834     21.0   8.14\n",
       "17   14.67  5.990     21.0   8.14\n",
       "18   11.69  5.456     21.0   8.14\n",
       "20   21.02  5.570     21.0   8.14\n",
       "21   13.83  5.965     21.0   8.14\n",
       "22   18.72  6.142     21.0   8.14\n",
       "23   19.88  5.813     21.0   8.14\n",
       "24   16.30  5.924     21.0   8.14\n",
       "26   14.81  5.813     21.0   8.14\n",
       "27   17.28  6.047     21.0   8.14\n",
       "28   12.80  6.495     21.0   8.14\n",
       "29   11.98  6.674     21.0   8.14\n",
       "30   22.60  5.713     21.0   8.14\n",
       "31   13.04  6.072     21.0   8.14\n",
       "33   18.35  5.701     21.0   8.14\n",
       "34   20.34  6.096     21.0   8.14\n",
       "35    9.68  5.933     19.2   5.96\n",
       "36   11.41  5.841     19.2   5.96\n",
       "37    8.77  5.850     19.2   5.96\n",
       "..     ...    ...      ...    ...\n",
       "465  14.13  5.759     20.2  18.10\n",
       "466  17.15  5.952     20.2  18.10\n",
       "468  18.13  5.926     20.2  18.10\n",
       "469  14.76  5.713     20.2  18.10\n",
       "470  16.29  6.167     20.2  18.10\n",
       "471  12.87  6.229     20.2  18.10\n",
       "472  14.36  6.437     20.2  18.10\n",
       "473  11.66  6.980     20.2  18.10\n",
       "476  18.68  6.484     20.2  18.10\n",
       "477  24.91  5.304     20.2  18.10\n",
       "479  13.11  6.229     20.2  18.10\n",
       "480  10.74  6.242     20.2  18.10\n",
       "481   7.74  6.750     20.2  18.10\n",
       "482   7.01  7.061     20.2  18.10\n",
       "484  13.34  5.871     20.2  18.10\n",
       "487  11.45  5.905     20.2  18.10\n",
       "488  18.06  5.454     20.1  27.74\n",
       "490  29.68  5.093     20.1  27.74\n",
       "492  13.35  5.983     20.1  27.74\n",
       "493  12.01  5.707     19.2   9.69\n",
       "494  13.59  5.926     19.2   9.69\n",
       "495  17.60  5.670     19.2   9.69\n",
       "496  21.14  5.390     19.2   9.69\n",
       "497  14.10  5.794     19.2   9.69\n",
       "498  12.92  6.019     19.2   9.69\n",
       "499  15.10  5.569     19.2   9.69\n",
       "500  14.33  6.027     19.2   9.69\n",
       "501   9.67  6.593     21.0  11.93\n",
       "504   6.48  6.794     21.0  11.93\n",
       "505   7.88  6.030     21.0  11.93\n",
       "\n",
       "[405 rows x 4 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.iloc[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1620, 405]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-a9fa8e6cc966>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mlinregkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0my_pred_kf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinregkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 463\u001b[0;31m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 205\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1620, 405]"
     ]
    }
   ],
   "source": [
    "linregkf = LinearRegression()\n",
    "mse_compare = []\n",
    "rmse_compare = []\n",
    "scores_compare = []\n",
    "\n",
    "for n in range(5,11):\n",
    "    kf = model_selection.KFold(n_splits=n, shuffle=True)\n",
    "    kf.split(X2,y)\n",
    "    \n",
    "    X_train, X_test = np.array(X2.iloc[train_index]).reshape(-1,1), np.array(X2.iloc[test_index]).reshape(-1,1)\n",
    "    #X_train, X_test = X2.iloc[train_index], X2.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    linregkf.fit(X_train,y_train)\n",
    "    y_pred_kf = linregkf.predict(X_test)\n",
    "    \n",
    "    mse_kf = mean_squared_error(y_pred_kf, y_test)\n",
    "    rmse_kf = np.sqrt(mse_kf)\n",
    "    score_kf = linregkf.score(y_pred_kf, y_test)\n",
    "    \n",
    "    mse_compare.append(mse_kf)\n",
    "    rmse_compare (rmse_kf)\n",
    "    scores_compare.append(score_kf)\n",
    "    \n",
    "    print(\"\\nNumber of splits:\",n, \"\\nScore:\", score_kf, \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I got the error above and couldn´t solve it. \n",
    "# instead, researching I found the cross_val_score which essentially does what I was intending to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_validate,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "linregkf = LinearRegression()\n",
    "\n",
    "scores = []\n",
    "avg_scores = []\n",
    "\n",
    "for n in range(5,11):\n",
    "    scores.append(cross_val_score(linregkf,X2,y, cv = n))\n",
    "    avg_scores.append(cross_val_score(linregkf,X2,y, cv = n).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.72738696,  0.70240317,  0.54242767,  0.15053352, -0.11775201]),\n",
       " array([ 0.71235875,  0.70824764,  0.50507743,  0.71664658, -0.0119341 ,\n",
       "         0.05647687]),\n",
       " array([ 0.73131524,  0.65012358,  0.57256464,  0.60858099,  0.25570083,\n",
       "        -0.15213022,  0.27370477]),\n",
       " array([ 0.75939364,  0.61034738,  0.67826647,  0.61518915,  0.70260846,\n",
       "        -0.30401301, -0.81490593,  0.28931779]),\n",
       " array([ 0.75508033,  0.66438045,  0.70818793,  0.45657467,  0.68427552,\n",
       "         0.58534465, -0.06501066, -0.94716384,  0.3201021 ]),\n",
       " array([ 0.76283488,  0.64503995, -0.58959275,  0.55381153,  0.6005876 ,\n",
       "         0.65748101,  0.00559449,  0.02921866, -1.20250812,  0.46737651])]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4009998623847066,\n",
       " 0.44781219526866484,\n",
       " 0.4199799747539271,\n",
       " 0.31702549344517644,\n",
       " 0.3513079052726408,\n",
       " 0.19298437674554275]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the more folds the lower my score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00375605, 0.00356984, 0.00353312, 0.00343394, 0.00398231]),\n",
       " 'score_time': array([0.00286913, 0.00468802, 0.00386405, 0.00445175, 0.00436568]),\n",
       " 'test_explained_variance': array([0.76132978, 0.70346185, 0.61996773, 0.15328271, 0.10203759]),\n",
       " 'test_r2': array([ 0.72738696,  0.70240317,  0.54242767,  0.15053352, -0.11775201])}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validate(linregkf,X2,y, cv = 5, scoring = [\"explained_variance\", \"r2\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the iris data into a DataFrame, take it from the notebook where we saw KNN\n",
    "#create \n",
    "#col_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.head()\n",
    "\n",
    "# Increase the default figure and font sizes for easier viewing\n",
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom colormap\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each iris species to a number\n",
    "# Let's use Iris-setosa':0, 'Iris-versicolor':1, 'Iris-virginica':2 and create a column called 'species_num'\n",
    "\n",
    "# Create a scatterplot of PETAL LENGTH versus PETAL WIDTH and color by SPECIES\n",
    "\n",
    "# Create a scatterplot of SEPAL LENGTH versus SEPAL WIDTH and color by SPECIES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classification of the Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your feature matrix \"X\"\n",
    "This will be all species measurements (sepal length, petal width, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your target vector \"y\"\n",
    "This will be the species type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Use of Train-Test-Split\n",
    "Split your data in to train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import KNN From `scikit-learn` and Instatiate a Model With One Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Accuracy\n",
    "Train your model using the training set then use the test set to determine the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Model With Five Neighbors. Did it Improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Looped Function That Will Check All Levels of Various Neighbors and Calculate the Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: According to `scikit-learn` Documentation, What is `knn.predict_proba(X_new)` Going to Do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enrichment\n",
    "_Everything beyond this point is enrichment and examples using Statsmodels for linear regression._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Using the Statsmodels Formula\n",
    "\n",
    "Adapt the formula example using your metrics. We will review this implementation in class. Here is a reference to consider. The workflow is the same, but the syntax is a little different. We want to get accustomed to the formula syntax because we will be using them a lot more with regressions. The results should be comparable to scikit-learn's regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, format our data in a DataFrame\n",
    "\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "df['MEDV'] = boston.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our new statsmodel.formula handling model\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# You can easily swap these out to test multiple versions/different formulas\n",
    "formulas = {\n",
    "    \"case1\": \"MEDV ~ RM + LSTAT + RAD + TAX + NOX + INDUS + CRIM + ZN - 1\", # - 1 = remove intercept\n",
    "    \"case2\": \"MEDV ~ NOX + RM\",\n",
    "    \"case3\": \"MEDV ~ RAD + TAX\"\n",
    "}\n",
    "\n",
    "model = smf.ols(formula=formulas['case1'], data=df)\n",
    "result = model.fit()\n",
    "\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Challenge #1:\n",
    "\n",
    "Can you optimize your R2, selecting the best features and using either test-train split or k-folds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Challenge #2:\n",
    "\n",
    "Given a combination of predictors, can you find another response variable that can be accurately predicted through the exploration of different predictors in this data set?\n",
    "\n",
    "_Tip: Check out pairplots, coefficients, and Pearson scores._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out variable relations\n",
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out Pearson scores\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
